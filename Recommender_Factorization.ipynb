{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 1000, number of users: 10000\n",
      "number of items: 1000, number of users: 10000\n",
      "(1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from pre_post_process import *\n",
    "\n",
    "_, ratings = load_data(\"data_train.csv\")\n",
    "sample_ids, _ = load_data(\"sample_submission.csv\")\n",
    "print(np.shape(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20bf9005ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 3, min # of users per item = 8.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (1000, 10000)\n",
      "the shape of valid ratings. (# of row, # of col): (1000, 10000)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1068598\n",
      "Total number of nonzero elements in test data:108354\n"
     ]
    }
   ],
   "source": [
    "valid_ratings, train, test = split_data(ratings, p_test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding parameters for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SGD_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the values of the step size $\\gamma$ first for a fixed value of the 3 other parameters and then compute a grid search to find the best parameters for the regularizers $\\lambda_{user}$, $\\lambda_{item}$ (both between 0 and 1) and the number of features $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0254163402880199.\n",
      "iter: 5, RMSE on training set: 0.9970562707025038.\n",
      "iter: 10, RMSE on training set: 0.9878420873516897.\n",
      "iter: 15, RMSE on training set: 0.9827800385900544.\n",
      "RMSE on test data: 0.9956946199719938.\n",
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0476258793239321.\n",
      "iter: 5, RMSE on training set: 0.9589571948307791.\n",
      "iter: 10, RMSE on training set: 0.8527435785856862.\n",
      "iter: 15, RMSE on training set: 0.8095947135380929.\n",
      "RMSE on test data: 1.0043973015749026.\n",
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0919266199950395.\n",
      "iter: 5, RMSE on training set: 0.7711472883038539.\n",
      "iter: 10, RMSE on training set: 0.6921353233596186.\n",
      "iter: 15, RMSE on training set: 0.6730727947086461.\n",
      "RMSE on test data: 1.1173694899753186.\n"
     ]
    }
   ],
   "source": [
    "# Finding gamma:\n",
    "gammas = np.logspace(-5,0,6) # errors on tests are [4.58, 1.43, 1.05, 1.02, nan, nan] Above 0.05 -> nan\n",
    "K = 50\n",
    "lambda_user = 0.01\n",
    "lambda_item = 0.01\n",
    "num_epochs = 20\n",
    "errors = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    # Initialize features matrix\n",
    "    user_init, item_init = init_MF(train, K)\n",
    "    # Compute SGD\n",
    "    _, _, rmse = matrix_factorization_SGD(train, test, gamma, K, lambda_user, lambda_item, num_epochs, user_init, item_init)\n",
    "    errors.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many computations (not only on logspace) for same parameters for K and the 2 lambdas, we found that $\\gamma = 0.025$ is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 20, lambda_u = 0.005, lambda_i = 0.05\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.05, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0319645103983506.\n",
      "iter: 5, RMSE on training set: 1.00386363668872.\n",
      "iter: 10, RMSE on training set: 0.9852919940605751.\n",
      "iter: 15, RMSE on training set: 0.9756309207073471.\n",
      "RMSE on test data: 0.9919069009109023.\n",
      "K = 20, lambda_u = 0.005, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0155132474468431.\n",
      "iter: 5, RMSE on training set: 0.9580145579687189.\n",
      "iter: 10, RMSE on training set: 0.9282677738283455.\n",
      "iter: 15, RMSE on training set: 0.9178277238362167.\n",
      "RMSE on test data: 0.9855366773648361.\n",
      "K = 20, lambda_u = 0.005, lambda_i = 0.5\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.5, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0411236927654304.\n",
      "iter: 5, RMSE on training set: 0.9932798549544953.\n",
      "iter: 10, RMSE on training set: 0.9559289502257527.\n",
      "iter: 15, RMSE on training set: 0.943369742156618.\n",
      "RMSE on test data: 0.9902358717422781.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.05\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.05, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0347572130248075.\n",
      "iter: 5, RMSE on training set: 0.9185363170899346.\n",
      "iter: 10, RMSE on training set: 0.8764839868882884.\n",
      "iter: 15, RMSE on training set: 0.8639836103991108.\n",
      "RMSE on test data: 1.0161567685436224.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.002164446269932.\n",
      "iter: 5, RMSE on training set: 0.9106104521342812.\n",
      "iter: 10, RMSE on training set: 0.8725916152355692.\n"
     ]
    }
   ],
   "source": [
    "# Grid Search:\n",
    "grid = np.zeros((4, 4, 4))\n",
    "gamma = 0.025 # best gamma we found above\n",
    "num_epochs = 20\n",
    "lambdas_user = [0.01, 0.05]#np.logspace(-3,0,4)[::-1] #From max to min\n",
    "lambdas_item = [0.05, 0.1, 0.5]\n",
    "num_features = np.linspace(20,100,4)\n",
    "min_loss = 100000\n",
    "\n",
    "for x,K in enumerate(num_features):\n",
    "    ### Warm start: directly start computation from previously computed item_features and user_features and not random initialization \n",
    "    user_init, item_init = init_MF(train, int(K))\n",
    "    for y,lambda_u in enumerate(lambdas_user):\n",
    "        for z,lambda_i in enumerate(lambdas_item):\n",
    "            print(\"K = {}, lambda_u = {}, lambda_i = {}\".format(int(K), lambda_u, lambda_i))\n",
    "            item_feats, user_feats, rmse = matrix_factorization_SGD(train, test, gamma, int(K), lambda_u, lambda_i, num_epochs,\n",
    "                                                                    user_init, item_init)\n",
    "            ### For warm start, we keep the user_features and item_features that gave us the minimal rmse previously computed\n",
    "            if rmse < min_loss:\n",
    "                min_loss = rmse\n",
    "                user_init = user_feats\n",
    "                item_init = item_feats\n",
    "            grid[x, y, z] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-83ef03b58d55>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-83ef03b58d55>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    t = np.matrix([[1.416,, 4],[1, 4,9, 16],[1, 8, 27, 64],[1, 16, 81, 100]])\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''TEST'''\n",
    "t = np.matrix([[1.416,, 4],[1, 4,9, 16],[1, 8, 27, 64],[1, 16, 81, 100]])\n",
    "ax = plt.axes()\n",
    "cmap = sns.color_palette(\"Blues\")\n",
    "sns.heatmap(t, ax = ax, cmap = cmap, square = True)\n",
    "lambdas_test1 = [0.001, 0.01, 0.1, 1.0]\n",
    "lambdas_test2 = [0.001, 0.01, 0.1, 1.0]\n",
    "ax.set_xticklabels(lambdas_test1)\n",
    "ax.set_yticklabels(lambdas_test2)\n",
    "ax.set_xlabel(\"Lambda user\")\n",
    "ax.set_ylabel(\"Lambda item\")\n",
    "plt.title(\"Grid Search for minimal RMSE depending on regularizers lambda user and lambda item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = np.matrix([[1.1,1.1,1.0836,0.9951],[1.0719, 1.0446, 0.9902, 1.0103],[1.009, 0.9855, 1.0043, 1.050],[1.007, 1.007, 1.048, 1.416]])\n",
    "ax = plt.axes()\n",
    "cmap = sns.palplot(sns.light_palette(\"red\", reverse=True))\n",
    "sns.heatmap(t, ax = ax, cmap = cmap, square = True)\n",
    "lambdas_test1 = [0.001, 0.01, 0.1, 1.0]\n",
    "lambdas_test2 = [0.001, 0.01, 0.1, 1.0]\n",
    "ax.set_xticklabels(lambdas_test1)\n",
    "ax.set_yticklabels(lambdas_test2)\n",
    "ax.set_xlabel(\"Lambda user\")\n",
    "ax.set_ylabel(\"Lambda item\")\n",
    "plt.title(\"Grid Search for minimal RMSE depending on regularizers lambda user and lambda item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the SGD with the best parameters we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 1.049493431379777.\n",
      "iter: 5, RMSE on training set: 0.9917019620172697.\n",
      "iter: 10, RMSE on training set: 0.9462150810318175.\n",
      "iter: 15, RMSE on training set: 0.9264185722848745.\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 1.0494871119227165.\n",
      "iter: 5, RMSE on training set: 0.9906779012935527.\n",
      "iter: 10, RMSE on training set: 0.9444948358625014.\n",
      "iter: 15, RMSE on training set: 0.924736391461175.\n",
      "iter: 20, RMSE on training set: 0.9177705836387646.\n",
      "iter: 25, RMSE on training set: 0.9152491052507084.\n",
      "iter: 30, RMSE on training set: 0.9142824072136536.\n",
      "iter: 35, RMSE on training set: 0.9139015618343987.\n",
      "iter: 40, RMSE on training set: 0.9137478691018356.\n",
      "iter: 45, RMSE on training set: 0.9136861943271423.\n",
      "iter: 50, RMSE on training set: 0.9136614109781991.\n",
      "iter: 55, RMSE on training set: 0.9136514502341064.\n",
      "iter: 60, RMSE on training set: 0.9136474475381103.\n",
      "iter: 65, RMSE on training set: 0.913645839048148.\n",
      "iter: 70, RMSE on training set: 0.9136451926342387.\n",
      "iter: 75, RMSE on training set: 0.9136449328553333.\n",
      "iter: 80, RMSE on training set: 0.9136448284560585.\n",
      "iter: 85, RMSE on training set: 0.9136447865003106.\n",
      "iter: 90, RMSE on training set: 0.9136447696392799.\n",
      "iter: 95, RMSE on training set: 0.9136447628631513.\n"
     ]
    }
   ],
   "source": [
    "best_gamma = 0.025\n",
    "best_lambda_u = 0.1\n",
    "best_lambda_i = 0.01\n",
    "K = 20\n",
    "num_epochs = 20\n",
    "\n",
    "user_init, item_init = init_MF(train, K)\n",
    "\n",
    "item_feats_SGD, user_feats_SGD, rmse = matrix_factorization_SGD(ratings, test, best_gamma, K, best_lambda_u, best_lambda_i, num_epochs,\n",
    "                                                                    user_init, item_init, include_test = False)\n",
    "user_init, item_init = init_MF(train, K)\n",
    "item_feats2_SGD, user_feats2_SGD, rmse = matrix_factorization_SGD(ratings, test, best_gamma, K, best_lambda_u, best_lambda_i, 100,\n",
    "                                                                    user_init, item_init, include_test = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions (user x items): (1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "predictions = np.dot(item_feats2_SGD.T, user_feats2_SGD)\n",
    "print(\"Shape of predictions (user x items): {}\".format(np.shape(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bias_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having $p_{u, i} = \\mathbf{w}_i\\mathbf{z^{T}}_u$ we would add bias on the user and item by having the following:\n",
    "$$p_{u, i} = \\mu + b_{u} + b_{i} + \\mathbf{w}_i\\mathbf{z^{T}}_u$$\n",
    "\n",
    "where $\\mu$ is the average of all ratings, $b_{u}$ and $b_{i}$ are the observed deviations of user u and item i respectively from the average (the biases).\n",
    "\n",
    "Thus we now want to find the best $\\mathbf{W}$ and $\\mathbf{U}$ that minimizes the loss:\n",
    "\n",
    "$$min_{W,Z}\\ \\frac{1}{2} \\sum_{(u, i) \\in \\omega} (r_{u,i} - \\mu - b_{u} - b_{i} - \\mathbf{W_{u}} \\mathbf{Z^{T}_{i}}) + \\lambda_{item} (||W||_{F}^{2} + b_{i}^{2}) + \\lambda_{user} (||Z||_{F}^{2} + b_{u}^{2}) $$\n",
    "\n",
    "And we need to compute the gradient of this loss. It is the same as before except we can convert our rating matrix to a biased rating matrix with ratings $r'_{u, i} = r_{u, i} - \\mu - b_{u} - b_{i}$ and compute our SGD on this biased matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\bias_helpers.py:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_users = [mean_users[0, i] for i in range(num_users)]/nz_users\n",
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\bias_helpers.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mean_items = [mean_items[i, 0] for i in range(num_items)]/nz_items\n"
     ]
    }
   ],
   "source": [
    "bias_train, mean, bias_u_train, bias_i_train = computeBiasMatrix(train) #ratings for final submissions\n",
    "bias_test, _, _, _ = computeBiasMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 20, lambda_u = 1.0, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950116460840475.\n",
      "iter: 5, RMSE on training set: 0.9950141371196208.\n",
      "iter: 10, RMSE on training set: 0.9950141496724862.\n",
      "iter: 15, RMSE on training set: 0.9950141514276349.\n",
      "RMSE on test data: 0.9584170940125771.\n",
      "Entered\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950141420014696.\n",
      "iter: 5, RMSE on training set: 0.9950141520396367.\n",
      "iter: 10, RMSE on training set: 0.9950141530242792.\n",
      "iter: 15, RMSE on training set: 0.995014153266552.\n",
      "RMSE on test data: 0.9584170952714977.\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950141512207128.\n",
      "iter: 5, RMSE on training set: 0.9950141258529559.\n",
      "iter: 10, RMSE on training set: 0.9950140651542321.\n",
      "iter: 15, RMSE on training set: 0.995014005162889.\n",
      "RMSE on test data: 0.9584169749498483.\n",
      "Entered\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950133195261307.\n",
      "iter: 5, RMSE on training set: 0.9948976959115556.\n",
      "iter: 10, RMSE on training set: 0.9940888464964817.\n",
      "iter: 15, RMSE on training set: 0.9929839885146264.\n",
      "RMSE on test data: 0.9565989281695966.\n",
      "Entered\n",
      "K = 20, lambda_u = 0.1, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9949680922788974.\n",
      "iter: 5, RMSE on training set: 0.994993637977154.\n",
      "iter: 10, RMSE on training set: 0.9949995426562931.\n",
      "iter: 15, RMSE on training set: 0.9950015132821844.\n",
      "RMSE on test data: 0.9584092112048314.\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9948250988024787.\n",
      "iter: 5, RMSE on training set: 0.9927056263748828.\n",
      "iter: 10, RMSE on training set: 0.9903822534585195.\n",
      "iter: 15, RMSE on training set: 0.9893716827917866.\n",
      "RMSE on test data: 0.9542685495751663.\n",
      "Entered\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9812590338416409.\n",
      "iter: 5, RMSE on training set: 0.9693179568486112.\n",
      "iter: 10, RMSE on training set: 0.9660966012174257.\n",
      "iter: 15, RMSE on training set: 0.9648296704733811.\n",
      "RMSE on test data: 0.9455059375553836.\n",
      "Entered\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.957897285749819.\n",
      "iter: 5, RMSE on training set: 0.9091981740989575.\n",
      "iter: 10, RMSE on training set: 0.8837000873739708.\n",
      "iter: 15, RMSE on training set: 0.8732654745835557.\n",
      "RMSE on test data: 0.9681576425317112.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.9882875207364366.\n",
      "iter: 5, RMSE on training set: 0.9847829518064954.\n",
      "iter: 10, RMSE on training set: 0.9836757360521378.\n",
      "iter: 15, RMSE on training set: 0.9833116582198246.\n",
      "RMSE on test data: 0.9532912553089247.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.9244031512459823.\n",
      "iter: 5, RMSE on training set: 0.8898694362997531.\n",
      "iter: 10, RMSE on training set: 0.8806801916409861.\n",
      "iter: 15, RMSE on training set: 0.8778351950937274.\n",
      "RMSE on test data: 0.9483331827082913.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.8519840326423436.\n",
      "iter: 5, RMSE on training set: 0.8322923035465783.\n",
      "iter: 10, RMSE on training set: 0.8274764814153364.\n",
      "iter: 15, RMSE on training set: 0.8260988240564066.\n",
      "RMSE on test data: 1.0109419764439198.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.8317292982008271.\n",
      "iter: 5, RMSE on training set: 0.8195943140974112.\n",
      "iter: 10, RMSE on training set: 0.8166055684124714.\n",
      "iter: 15, RMSE on training set: 0.8158170370029025.\n",
      "RMSE on test data: 1.0423736827911525.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.9587480518349353.\n",
      "iter: 5, RMSE on training set: 0.9435593422129767.\n",
      "iter: 10, RMSE on training set: 0.938499539282763.\n",
      "iter: 15, RMSE on training set: 0.9367760957181325.\n",
      "RMSE on test data: 0.9432754014640384.\n",
      "Entered\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.8630054952523196.\n",
      "iter: 5, RMSE on training set: 0.8445146230500347.\n",
      "iter: 10, RMSE on training set: 0.8365844285949335.\n",
      "iter: 15, RMSE on training set: 0.8345578783121249.\n",
      "RMSE on test data: 0.989899321704543.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.8746704288655027.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\SGD_helpers.py:53: RuntimeWarning: overflow encountered in multiply\n",
      "  user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\SGD_helpers.py:52: RuntimeWarning: overflow encountered in multiply\n",
      "  item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\SGD_helpers.py:53: RuntimeWarning: invalid value encountered in subtract\n",
      "  user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
      "C:\\Users\\olivi\\Documents\\semestre7\\MachineLearning\\ML_course\\projects\\project2\\project_recommender_system\\SGD_helpers.py:52: RuntimeWarning: invalid value encountered in add\n",
      "  item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 5, RMSE on training set: nan.\n",
      "iter: 10, RMSE on training set: nan.\n",
      "iter: 15, RMSE on training set: nan.\n",
      "RMSE on test data: nan.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: nan.\n",
      "iter: 5, RMSE on training set: nan.\n",
      "iter: 10, RMSE on training set: nan.\n",
      "iter: 15, RMSE on training set: nan.\n",
      "RMSE on test data: nan.\n"
     ]
    }
   ],
   "source": [
    "# Grid Search:\n",
    "grid = np.zeros((4, 4)) ### np.zeros((4, 4, 4))\n",
    "gamma = 0.025 # best gamma we found above\n",
    "num_epochs = 20\n",
    "lambdas_user = np.logspace(-3,0,4)[::-1] #From max to min\n",
    "lambdas_item = np.logspace(-3,0,4)[::-1]\n",
    "#num_features = np.linspace(20,100,4)\n",
    "K = 20\n",
    "min_loss = 100000\n",
    "\n",
    "### Warm start: directly start computation from previously computed item_features and user_features and not random initialization \n",
    "user_init, item_init = init_MF(train, K)\n",
    "for x,lambda_u in enumerate(lambdas_user):\n",
    "    for y,lambda_i in enumerate(lambdas_item):\n",
    "        print(\"K = {}, lambda_u = {}, lambda_i = {}\".format(int(K), lambda_u, lambda_i))\n",
    "        item_feats, user_feats, rmse = matrix_factorization_SGD(bias_train, bias_test, gamma, K, lambda_u,\n",
    "                                                                lambda_i, num_epochs, user_init, item_init)\n",
    "        ### For warm start, we keep the user_features and item_features that gave us the minimal rmse previously computed\n",
    "        if rmse < min_loss:\n",
    "            print(\"Entered\")\n",
    "            min_loss = rmse\n",
    "            user_init = user_feats\n",
    "            item_init = item_feats\n",
    "        grid[x, y] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.9950023491148128.\n",
      "iter: 5, RMSE on training set: 0.9949891307219013.\n",
      "iter: 10, RMSE on training set: 0.9949801034480176.\n",
      "iter: 15, RMSE on training set: 0.9949762003882027.\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "gamma = 0.025\n",
    "K = 20\n",
    "lambda_user = 0.001\n",
    "lambda_item = 1.0\n",
    "num_epochs = 20\n",
    "user_init, item_init = init_MF(bias_train, K)\n",
    "\n",
    "item_featuresSGD, user_featuresSGD, rmse = matrix_factorization_SGD(bias_train, bias_test, gamma, K, lambda_user,\n",
    "                                                              lambda_item, num_epochs, user_init, item_init, include_test = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10000)\n",
      "Shape of predictions (user x items): (1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions matrix from the biases, item and user features computed with SGD\n",
    "predictions = predictionsWithBias(item_featuresSGD, user_featuresSGD, bias_u_train, bias_i_train, mean)\n",
    "print(\"Shape of predictions (user x items): {}\".format(np.shape(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 37\n",
      "3.3532282857\n"
     ]
    }
   ],
   "source": [
    "### Checking if results appear the same in the final excel file\n",
    "first_user, first_item = sample_ids[0][0],sample_ids[0][1]\n",
    "print(first_item, first_user)\n",
    "print(predictions[0, 36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions (user x items): (1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of predictions (user x items): {}\".format(np.shape(predictions)))\n",
    "wanted_preds = getWantedPredictions(predictions.T, sample_ids)\n",
    "create_csv_submission(sample_ids, wanted_preds, \"submissions/SGD_bias_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
