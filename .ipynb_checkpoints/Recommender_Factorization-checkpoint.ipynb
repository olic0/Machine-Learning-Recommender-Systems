{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Note that `ratings` is a sparse matrix that in the shape of (num_items, num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 1000, number of users: 10000\n",
      "number of items: 1000, number of users: 10000\n",
      "(1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "from pre_post_process import * #load_data, preprocess_data\n",
    "\n",
    "_, ratings = load_data(\"data_train.csv\")\n",
    "sample_ids, _ = load_data(\"sample_submission.csv\")\n",
    "print(np.shape(ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the number of ratings per movie and user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x217e5ca8898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 3, min # of users per item = 8.\n"
     ]
    }
   ],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, p_test=0.1):\n",
    "    \"\"\"Split the ratings to training data and test data with the given proportion in p_test\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    valid_ratings = ratings\n",
    "    \n",
    "    # init\n",
    "    num_rows, num_cols = valid_ratings.shape\n",
    "    train = sp.lil_matrix((num_rows, num_cols))\n",
    "    test = sp.lil_matrix((num_rows, num_cols))\n",
    "    \n",
    "    print(\"the shape of original ratings. (# of row, # of col): {}\".format(\n",
    "        ratings.shape))\n",
    "    print(\"the shape of valid ratings. (# of row, # of col): {}\".format(\n",
    "        (num_rows, num_cols)))\n",
    "\n",
    "    nz_items, nz_users = valid_ratings.nonzero()\n",
    "    \n",
    "    # split the data\n",
    "    for user in set(nz_users):\n",
    "        # randomly select a subset of ratings\n",
    "        row, col = valid_ratings[:, user].nonzero()\n",
    "        selects = np.random.choice(row, size=int(len(row) * p_test))\n",
    "        residual = list(set(row) - set(selects))\n",
    "\n",
    "        # add to train set\n",
    "        train[residual, user] = valid_ratings[residual, user]\n",
    "\n",
    "        # add to test set\n",
    "        test[selects, user] = valid_ratings[selects, user]\n",
    "\n",
    "    print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2e2ddb673fc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from pre_post_process import split_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvalid_ratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-50a24061c258>\u001b[0m in \u001b[0;36msplit_data\u001b[1;34m(ratings, p_test)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_ratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "#from pre_post_process import split_data\n",
    "valid_ratings, train, test = split_data(ratings, p_test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Matrix Factorization using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "        \n",
    "    num_item, num_user = train.get_shape()\n",
    "\n",
    "    user_features = np.random.rand(num_features, num_user)/num_features\n",
    "    item_features = np.random.rand(num_features, num_item)/num_features\n",
    "\n",
    "    # start by item features.\n",
    "    item_nnz = train.getnnz(axis=1)\n",
    "    item_sum = train.sum(axis=1)\n",
    "\n",
    "    #for ind in range(num_item):\n",
    "        #item_features[0, ind] = item_sum[ind, 0] / item_nnz[ind]\n",
    "    return user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cost by the method of matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    mse = 0\n",
    "    for row, col in nz:\n",
    "        item_info = item_features[:, row]\n",
    "        user_info = user_features[:, col]\n",
    "        mse += (data[row, col] - user_info.T.dot(item_info)) ** 2\n",
    "    return np.sqrt(1.0 * mse / len(nz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test, gamma, num_features, lambda_user, lambda_item, num_epochs,\n",
    "                             user_features, item_features, include_test = True):\n",
    "    \"\"\"matrix factorization by SGD. None for user_feats and item_feats only if warm_start is False\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_test = []\n",
    "    if(include_test):\n",
    "        nz_row, nz_col = test.nonzero()\n",
    "        nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"Learn the matrix factorization using SGD with K = {}, lambda_i = {}, lambda_u = {}\".format(num_features, lambda_item, lambda_user))\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "            # update W_d (item_features[:, d]) and Z_n (user_features[:, n])\n",
    "            item_info = item_features[:, d]\n",
    "            user_info = user_features[:, n]\n",
    "            err = train[d, n] - user_info.T.dot(item_info)\n",
    "            # calculate the gradient and update\n",
    "            item_features[:, d] += gamma * (err * user_info - lambda_item * item_info)\n",
    "            user_features[:, n] += gamma * (err * item_info - lambda_user * user_info)\n",
    "            \n",
    "        rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        if(it % 5 == 0):\n",
    "            print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "\n",
    "    # evaluate the test error\n",
    "    if include_test:\n",
    "        rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "        print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return item_features, user_features, rmse   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding parameters for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SGD_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the values of the step size $\\gamma$ first for a fixed value of the 3 other parameters and then compute a grid search to find the best parameters for the regularizers $\\lambda_{user}$, $\\lambda_{item}$ (both between 0 and 1) and the number of features $K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0254163402880199.\n",
      "iter: 5, RMSE on training set: 0.9970562707025038.\n",
      "iter: 10, RMSE on training set: 0.9878420873516897.\n",
      "iter: 15, RMSE on training set: 0.9827800385900544.\n",
      "RMSE on test data: 0.9956946199719938.\n",
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0476258793239321.\n",
      "iter: 5, RMSE on training set: 0.9589571948307791.\n",
      "iter: 10, RMSE on training set: 0.8527435785856862.\n",
      "iter: 15, RMSE on training set: 0.8095947135380929.\n",
      "RMSE on test data: 1.0043973015749026.\n",
      "Learn the matrix factorization using SGD with K = 50, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0919266199950395.\n",
      "iter: 5, RMSE on training set: 0.7711472883038539.\n",
      "iter: 10, RMSE on training set: 0.6921353233596186.\n",
      "iter: 15, RMSE on training set: 0.6730727947086461.\n",
      "RMSE on test data: 1.1173694899753186.\n"
     ]
    }
   ],
   "source": [
    "# Finding gamma:\n",
    "\n",
    "gammas = np.logspace(-5,0,6) # errors on tests are [4.58, 1.43, 1.05, 1.02, nan, nan] Above 0.05 -> nan\n",
    "K = 50\n",
    "lambda_user = 0.01\n",
    "lambda_item = 0.01\n",
    "num_epochs = 20\n",
    "errors = []\n",
    "\n",
    "for gamma in gammas:\n",
    "    user_init, item_init = init_MF(train, K)\n",
    "    _, _, rmse = matrix_factorization_SGD(train, test, gamma, K, lambda_user, lambda_item, num_epochs, user_init, item_init)\n",
    "    errors.append(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many computations (not only on logspace) for same parameters for K and the 2 lambdas, we found that $\\gamma = 0.025$ is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 20, lambda_u = 0.005, lambda_i = 0.05\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.05, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0319645103983506.\n",
      "iter: 5, RMSE on training set: 1.00386363668872.\n",
      "iter: 10, RMSE on training set: 0.9852919940605751.\n",
      "iter: 15, RMSE on training set: 0.9756309207073471.\n",
      "RMSE on test data: 0.9919069009109023.\n",
      "K = 20, lambda_u = 0.005, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0155132474468431.\n",
      "iter: 5, RMSE on training set: 0.9580145579687189.\n",
      "iter: 10, RMSE on training set: 0.9282677738283455.\n",
      "iter: 15, RMSE on training set: 0.9178277238362167.\n",
      "RMSE on test data: 0.9855366773648361.\n",
      "K = 20, lambda_u = 0.005, lambda_i = 0.5\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.5, lambda_u = 0.005\n",
      "iter: 0, RMSE on training set: 1.0411236927654304.\n",
      "iter: 5, RMSE on training set: 0.9932798549544953.\n",
      "iter: 10, RMSE on training set: 0.9559289502257527.\n",
      "iter: 15, RMSE on training set: 0.943369742156618.\n",
      "RMSE on test data: 0.9902358717422781.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.05\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.05, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.0347572130248075.\n",
      "iter: 5, RMSE on training set: 0.9185363170899346.\n",
      "iter: 10, RMSE on training set: 0.8764839868882884.\n",
      "iter: 15, RMSE on training set: 0.8639836103991108.\n",
      "RMSE on test data: 1.0161567685436224.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 1.002164446269932.\n",
      "iter: 5, RMSE on training set: 0.9106104521342812.\n",
      "iter: 10, RMSE on training set: 0.8725916152355692.\n"
     ]
    }
   ],
   "source": [
    "# Grid Search:\n",
    "grid = np.zeros((4, 4, 4)) ### np.zeros((4, 4, 4))\n",
    "gamma = 0.025 # best gamma we found above\n",
    "num_epochs = 20\n",
    "lambdas_user = [0.005, 0.01, 0.05]#np.logspace(-3,0,4)[::-1] #From max to min\n",
    "lambdas_item = [0.05, 0.1, 0.5]\n",
    "num_features = np.linspace(20,100,4)\n",
    "min_loss = 100000\n",
    "\n",
    "for x,K in enumerate(num_features):\n",
    "    ### Warm start: directly start computation from previously computed item_features and user_features and not random initialization \n",
    "    user_init, item_init = init_MF(train, int(K))\n",
    "    for y,lambda_u in enumerate(lambdas_user):\n",
    "        for z,lambda_i in enumerate(lambdas_item):\n",
    "            print(\"K = {}, lambda_u = {}, lambda_i = {}\".format(int(K), lambda_u, lambda_i))\n",
    "            item_feats, user_feats, rmse = matrix_factorization_SGD(train, test, gamma, int(K), lambda_u, lambda_i, num_epochs,\n",
    "                                                                    user_init, item_init)\n",
    "            ### For warm start, we keep the user_features and item_features that gave us the minimal rmse previously computed\n",
    "            if rmse < min_loss:\n",
    "                min_loss = rmse\n",
    "                user_init = user_feats\n",
    "                item_init = item_feats\n",
    "            grid[x, y, z] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xc368f19828>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAEWCAYAAABfQiwxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc368e2d3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''TEST'''\n",
    "t = np.matrix([[1, 2, 3, 4],[1, 4, 9, 16],[1, 8, 27, 64],[1, 16, 81, 100]])\n",
    "ax = plt.axes()\n",
    "cmap = sns.color_palette(\"Blues\")\n",
    "sns.heatmap(t, ax = ax, cmap = cmap, square = True)\n",
    "lambdas_test1 = [0.0001, 0.001, 0.01, 0.1]\n",
    "lambdas_test2 = [0.0001, 0.001, 0.01, 0.1]\n",
    "ax.set_xticklabels(lambdas_test1)\n",
    "ax.set_yticklabels(lambdas_test2)\n",
    "ax.set_xlabel(\"Lambda user\")\n",
    "ax.set_ylabel(\"Lambda item\")\n",
    "plt.title(\"Grid Search for minimal RMSE depending on regularizers lambda user and lambda item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the SGD with the best parameters we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_gamma = 0.025\n",
    "best_lambda_u = 0.1\n",
    "best_lambda_i = 0.01\n",
    "K = 20\n",
    "num_epochs = 20\n",
    "\n",
    "user_init, item_init = init_MF(train, K)\n",
    "\n",
    "item_feats_SGD, user_feats_SGD, rmse = matrix_factorization_SGD(ratings, test, best_gamma, K, best_lambda_u, best_lambda_i, num_epochs,\n",
    "                                                                    user_init, item_init, include_test = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bias_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having $p_{u, i} = \\mathbf{w}_i\\mathbf{z^{T}}_u$ we would add bias on the user and item by having the following:\n",
    "$$p_{u, i} = \\mu + b_{u} + b_{i} + \\mathbf{w}_i\\mathbf{z^{T}}_u$$\n",
    "\n",
    "where $\\mu$ is the average of all ratings, $b_{u}$ and $b_{i}$ are the observed deviations of user u and item i respectively from the average (the biases).\n",
    "\n",
    "Thus we now want to find the best $\\mathbf{W}$ and $\\mathbf{U}$ that minimizes the loss:\n",
    "\n",
    "$$min_{W,Z}\\ \\frac{1}{2} \\sum_{(u, i) \\in \\omega} (r_{u,i} - \\mu - b_{u} - b_{i} - \\mathbf{W_{u}} \\mathbf{Z^{T}_{i}}) + \\lambda_{item} (||W||_{F}^{2} + b_{i}^{2}) + \\lambda_{user} (||Z||_{F}^{2} + b_{u}^{2}) $$\n",
    "\n",
    "And we need to compute the gradient of this loss. It is the same as before except we can convert our rating matrix to a biased rating matrix with ratings $r'_{u, i} = r_{u, i} - \\mu - b_{u} - b_{i}$ and compute our SGD on this biased matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratings is matrix of dimension: items x users\n",
    "def computeBiasMatrix(ratings):\n",
    "    \"\"\"Compute the bias matrix of same dimension as the ratings matrix\n",
    "    where each element is equal to r_ui - mu - bias_u - bias_i\"\"\"\n",
    "    num_items, num_users = ratings.shape\n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = ratings.nonzero()\n",
    "    nz_ratings = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    # mu is overall mean (of non zeros)\n",
    "    mu = np.sum(ratings)/len(nz_ratings)\n",
    "\n",
    "    nz_users = np.zeros((num_users,), dtype=int)\n",
    "    nz_items = np.zeros((num_items,), dtype=int)\n",
    "    # Compute number of non zero ratings for each user and item\n",
    "    for item, user in nz_ratings:\n",
    "        nz_users[user] += 1\n",
    "        nz_items[item] += 1\n",
    "    \n",
    "    mean_users = np.sum(ratings, axis = 0)\n",
    "    mean_users = [mean_users[0, i] for i in range(num_users)]/nz_users\n",
    "    mean_items = np.sum(ratings, axis = 1)\n",
    "    mean_items = [mean_items[i, 0] for i in range(num_items)]/nz_items\n",
    "    \n",
    "    bias_users = mean_users - np.ones(num_users) * mu\n",
    "    bias_items = mean_items - np.ones(num_items) * mu\n",
    "    new_ratings = sp.lil_matrix((num_items, num_users))\n",
    "    for item, user in nz_ratings:\n",
    "        new_ratings[item, user] = ratings[item, user] - (mu + bias_users[user] + bias_items[item])\n",
    "    return new_ratings, mu, bias_users, bias_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "bias_train, mean, bias_u_train, bias_i_train = computeBiasMatrix(train) #ratings\n",
    "bias_test, _, _, _ = computeBiasMatrix(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 20, lambda_u = 1.0, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950116720477675.\n",
      "iter: 5, RMSE on training set: 0.9950141369933406.\n",
      "iter: 10, RMSE on training set: 0.9950141496906598.\n",
      "iter: 15, RMSE on training set: 0.9950141514729475.\n",
      "RMSE on test data: 0.9584170939579535.\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950141420859426.\n",
      "iter: 5, RMSE on training set: 0.9950141520257917.\n",
      "iter: 10, RMSE on training set: 0.9950141530127758.\n",
      "iter: 15, RMSE on training set: 0.9950141532576683.\n",
      "RMSE on test data: 0.9584170952633824.\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950141511814983.\n",
      "iter: 5, RMSE on training set: 0.9950141262925755.\n",
      "iter: 10, RMSE on training set: 0.9950140676593487.\n",
      "iter: 15, RMSE on training set: 0.9950140099869278.\n",
      "RMSE on test data: 0.958416979291496.\n",
      "K = 20, lambda_u = 1.0, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 1.0\n",
      "iter: 0, RMSE on training set: 0.9950133522072887.\n",
      "iter: 5, RMSE on training set: 0.9949032287442016.\n",
      "iter: 10, RMSE on training set: 0.9941313706738808.\n",
      "iter: 15, RMSE on training set: 0.9930687415120875.\n",
      "RMSE on test data: 0.9566685868455376.\n",
      "K = 20, lambda_u = 0.1, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9949699666394071.\n",
      "iter: 5, RMSE on training set: 0.9949944740484968.\n",
      "iter: 10, RMSE on training set: 0.995000141072254.\n",
      "iter: 15, RMSE on training set: 0.9950020276974609.\n",
      "RMSE on test data: 0.9584095284615755.\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9948328726003601.\n",
      "iter: 5, RMSE on training set: 0.9927857638304549.\n",
      "iter: 10, RMSE on training set: 0.9904955884695801.\n",
      "iter: 15, RMSE on training set: 0.9894846787276742.\n",
      "RMSE on test data: 0.9543411204962421.\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9813451402409408.\n",
      "iter: 5, RMSE on training set: 0.9717386288208021.\n",
      "iter: 10, RMSE on training set: 0.9665359860459964.\n",
      "iter: 15, RMSE on training set: 0.964823078786013.\n",
      "RMSE on test data: 0.9453717929758166.\n",
      "K = 20, lambda_u = 0.1, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.1\n",
      "iter: 0, RMSE on training set: 0.9567278750817676.\n",
      "iter: 5, RMSE on training set: 0.9102203673992817.\n",
      "iter: 10, RMSE on training set: 0.8838284303162773.\n",
      "iter: 15, RMSE on training set: 0.8729639918665925.\n",
      "RMSE on test data: 0.9680425862442609.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.9883004852445127.\n",
      "iter: 5, RMSE on training set: 0.9847936770808869.\n",
      "iter: 10, RMSE on training set: 0.9836791764338765.\n",
      "iter: 15, RMSE on training set: 0.9833119354268203.\n",
      "RMSE on test data: 0.9533250250281449.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.924115923823296.\n",
      "iter: 5, RMSE on training set: 0.8893150526036965.\n",
      "iter: 10, RMSE on training set: 0.8801413249982132.\n",
      "iter: 15, RMSE on training set: 0.8773049341260845.\n",
      "RMSE on test data: 0.9486327173950324.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.8517614663344333.\n",
      "iter: 5, RMSE on training set: 0.8322137682097144.\n",
      "iter: 10, RMSE on training set: 0.8274164068930059.\n",
      "iter: 15, RMSE on training set: 0.8260340337993763.\n",
      "RMSE on test data: 1.0122680303562075.\n",
      "K = 20, lambda_u = 0.01, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.01\n",
      "iter: 0, RMSE on training set: 0.831528135910676.\n",
      "iter: 5, RMSE on training set: 0.8195025738910826.\n",
      "iter: 10, RMSE on training set: 0.8165281577682876.\n",
      "iter: 15, RMSE on training set: 0.8157399614796698.\n",
      "RMSE on test data: 1.0446486316257588.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 1.0\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.9589590798340594.\n",
      "iter: 5, RMSE on training set: 0.9436065043713214.\n",
      "iter: 10, RMSE on training set: 0.9385108549558221.\n",
      "iter: 15, RMSE on training set: 0.9367661329583973.\n",
      "RMSE on test data: 0.9435385161638142.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.1\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.1, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.8626828534570321.\n",
      "iter: 5, RMSE on training set: 0.8443523470597768.\n",
      "iter: 10, RMSE on training set: 0.8365169006712216.\n",
      "iter: 15, RMSE on training set: 0.8344543919598093.\n",
      "RMSE on test data: 0.9914478431273162.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.01\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.01, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.8710201518083586.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:29: RuntimeWarning: overflow encountered in multiply\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:29: RuntimeWarning: invalid value encountered in add\n",
      "C:\\Users\\olivi\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:30: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 5, RMSE on training set: nan.\n",
      "iter: 10, RMSE on training set: nan.\n",
      "iter: 15, RMSE on training set: nan.\n",
      "RMSE on test data: nan.\n",
      "K = 20, lambda_u = 0.001, lambda_i = 0.001\n",
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 0.001, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: nan.\n",
      "iter: 5, RMSE on training set: nan.\n",
      "iter: 10, RMSE on training set: nan.\n",
      "iter: 15, RMSE on training set: nan.\n",
      "RMSE on test data: nan.\n"
     ]
    }
   ],
   "source": [
    "# Grid Search:\n",
    "grid = np.zeros((4, 4)) ### np.zeros((4, 4, 4))\n",
    "gamma = 0.025 # best gamma we found above\n",
    "num_epochs = 20\n",
    "lambdas_user = np.logspace(-3,0,4)[::-1] #From max to min\n",
    "lambdas_item = np.logspace(-3,0,4)[::-1]\n",
    "#num_features = np.linspace(20,100,4)\n",
    "K = 20\n",
    "min_loss = 100000\n",
    "\n",
    "### Warm start: directly start computation from previously computed item_features and user_features and not random initialization \n",
    "user_init, item_init = init_MF(train, K)\n",
    "for x,lambda_u in enumerate(lambdas_user):\n",
    "    for y,lambda_i in enumerate(lambdas_item):\n",
    "        print(\"K = {}, lambda_u = {}, lambda_i = {}\".format(int(K), lambda_u, lambda_i))\n",
    "        item_feats, user_feats, rmse = matrix_factorization_SGD(bias_train, bias_test, gamma, K, lambda_u,\n",
    "                                                                lambda_i, num_epochs, user_init, item_init)\n",
    "        ### For warm start, we keep the user_features and item_features that gave us the minimal rmse previously computed\n",
    "        if rmse < min_loss:\n",
    "            min_loss = rmse\n",
    "            user_init = user_feats\n",
    "            item_init = item_feats\n",
    "        grid[x, y] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the matrix factorization using SGD with K = 20, lambda_i = 1.0, lambda_u = 0.001\n",
      "iter: 0, RMSE on training set: 0.9953119278759404.\n",
      "iter: 5, RMSE on training set: 0.9952922850679001.\n",
      "iter: 10, RMSE on training set: 0.9952833079852658.\n",
      "iter: 15, RMSE on training set: 0.9952781573967916.\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "gamma = 0.025\n",
    "K = 20\n",
    "lambda_user = 0.001\n",
    "lambda_item = 1.0\n",
    "num_epochs = 20\n",
    "user_init, item_init = init_MF(bias_train, K)\n",
    "\n",
    "item_featuresSGD, user_featuresSGD, rmse = matrix_factorization_SGD(bias_train, bias_test, gamma, K, lambda_user,\n",
    "                                                              lambda_item, num_epochs, user_init, item_init, include_test = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 0.8 secs to compute\n",
    "def predictionsWithBias(item_features, user_features, bias_u, bias_i, mean_rating):\n",
    "    '''Computes the final predictions matrix using the formula: pred_u_i = mean + bias_u + bias_i + item_features.T @ user_features'''\n",
    "    # item_features and user_features of shape: K x num_items/users\n",
    "    preds_matrix = np.dot(item_features.T, user_features)\n",
    "    num_items, num_users = preds_matrix.shape\n",
    "    ### biasU: each elements of a column has same value (corresponding to the user bias)\n",
    "    biasU = np.tile(bias_u, (num_items,1))\n",
    "    ### biasI: each elements of a row has same value (corresponding to the movie bias)\n",
    "    biasI = np.tile(bias_i, (num_users,1)).T\n",
    "    mean_matrix = np.ones((num_items, num_users)) * mean_rating\n",
    "    \n",
    "    predictionsBiased = mean_matrix + biasU + biasI + preds_matrix\n",
    "    print(np.shape(predictionsBiased))\n",
    "    return predictionsBiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10000)\n",
      "2.685866355895996\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "predictions = predictionsWithBias(item_featuresSGD, user_featuresSGD, bias_u_train, bias_i_train, mean)\n",
    "end_time = time.time() - start_time\n",
    "print(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 37\n",
      "3.27873618137\n"
     ]
    }
   ],
   "source": [
    "### Checking if results appear the same in the final excel file\n",
    "first_user, first_item = sample_ids[0][0],sample_ids[0][1]\n",
    "print(first_item, first_user)\n",
    "print(predictions[0, 36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWantedPredictions(predictions_matrix, ids):\n",
    "    \"\"\"Computes array of the wanted predictions given the list of ids of the form [user_id, movie_id]\"\"\"\n",
    "    wanted_predictions = []\n",
    "    for user, movie in ids:\n",
    "        wanted_predictions.append(predictions_matrix[user - 1, movie - 1])\n",
    "    return wanted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10000) (1000, 20)\n",
      "Shape of predictions (user x items): (10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "### We want our matrix of the form user x movies: user_features (shape users x k) @ item_features.T (shape k x items)\n",
    "print(np.shape(user_feats_SGD), np.shape(item_feats_SGD.T))\n",
    "predictions = np.dot(item_feats_SGD.T, user_feats_SGD).T\n",
    "print(\"Shape of predictions (user x items): {}\".format(np.shape(predictions)))\n",
    "#predictions = predictionsWithBias(predictions, bias_users, bias_items).T\n",
    "wanted_preds = getWantedPredictions(predictions, sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(predictions))\n",
    "wanted_preds = getWantedPredictions(predictions.T, sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(sample_ids, wanted_preds, \"submissions/SGD_bias_best_param2_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
